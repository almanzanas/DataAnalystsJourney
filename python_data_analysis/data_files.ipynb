{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Storage, and File Formats\n",
    "\n",
    "*Data loading* term refers to read data and making it accessible. *Parsing* is also used to describe loading text data and interpreting it as tables and different data types.\n",
    "\n",
    "## Index\n",
    "\n",
    "* [Reading and Writing Data in Text Format](#reading-and-writing-data-in-text-format)\n",
    "    * [pandas.read_csv Functions](#some-pandasread_csv-function-arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data in Text Format\n",
    "\n",
    "*Text and binary data loading functions in pandas*\n",
    "|Function|Description|\n",
    "|---|---|\n",
    "|**read_csv** |**Load delimited data from a file, URL, or file-like object; use comma as default delimiter**|\n",
    "|read_fwf |Read data in fixed-width column format (i.e., no delimiters)|\n",
    "|read_clipboard |Variation of read_csv that reads data from the clipboard; useful for converting tables from web pages|\n",
    "|read_excel |Read tabular data from an Excel XLS or XLSX file|\n",
    "|read_hdf |Read HDF5 files written by pandas|\n",
    "|read_html |Read all tables found in the given HTML document|\n",
    "|**read_json** |**Read data from a JSON (JavaScript Object Notation) string representation, file, URL, or file-like object**|\n",
    "|read_feather |Read the Feather binary file format|\n",
    "|read_orc |Read the Apache ORC binary file format|\n",
    "|read_parquet |Read the Apache Parquet binary file format|\n",
    "|read_pickle |Read an object stored by pandas using the Python pickle format|\n",
    "|read_sas |Read a SAS dataset stored in one of the SAS systemâ€™s custom storage formats|\n",
    "|**read_spss** |Read a data file created by SPSS|\n",
    "|read_sql |Read the results of a SQL query (using SQLAlchemy)|\n",
    "|read_sql_table |Read a whole SQL table (using SQLAlchemy); equivalent to using a query that selects everything in that table using read_sql|\n",
    "|read_stata |Read a dataset from Stata file format|\n",
    "|read_xml |Read a table of data from an XML file|\n",
    "\n",
    "Some of this functions has a long list of optional arguments, `pandas.read_csv()` has around 50, so ig you are struggling to read a particular file you can look online to found your optimal arguments.\n",
    "\n",
    "```python\n",
    "# some examples:\n",
    "\n",
    "# The csv file has not headers, it will read with default names\n",
    "pd.read_csv(\"example.csv\", header=None)\n",
    "# or you can specify names\n",
    "pd.read_csv(\"example.csv\", names[\"a\", \"b\", \"c\", \"d\", \"message\"],\n",
    "            index_col=\"message\")\n",
    "# the argument 'index_col=\"message\"' to indicate your index column\n",
    "\n",
    "# Pass multiple col names (list) for a hierarchical index\n",
    "\n",
    "# You can pass a regular expression as delimeter for pandas\n",
    "# use sep=\"\\s+\" if the file are separated for a variable amount of whitespaces\n",
    "\n",
    "# with skiprows=[2, 3, 5] you can skip that rows\n",
    "pd.read_csv(\"example.csv\", skiprows=[2, 3, 5])\n",
    "```\n",
    "\n",
    "#### *Some pandas.read_csv function arguments*\n",
    "|Argument|Description|\n",
    "|---|---|\n",
    "|path |String indicating filesystem location, URL, or file-like object.|\n",
    "|sep or delimiter |Character sequence or regular expression to use to split fields in each row.|\n",
    "|header |Row number to use as column names; defaults to 0 (first row), but should be None if there is no header row.|\n",
    "|index_col |Column numbers or names to use as the row index in the result; can be a single name/number or a list of them for a hierarchical index.|\n",
    "|names |List of column names for result.|\n",
    "|skiprows| Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip.|\n",
    "|na_values |Sequence of values to replace with NA. They are added to the default list unless keep_default_na=False is passed.|\n",
    "|keep_default_na |Whether to use the default NA value list or not (True by default).|\n",
    "|comment| Character(s) to split comments off the end of lines.|\n",
    "|parse_dates |Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise, can specify a list of column numbers or names to parse. If element of list is tuple or list, will combine multiple columns together and parse to date (e.g., if date/time split across two columns).|\n",
    "|keep_date_col |If joining columns to parse date, keep the joined columns; False by default.|\n",
    "|converters |Dictionary containing column number or name mapping to functions (e.g., {\"foo\": f} would apply the function f to all values in the \"foo\" column).|\n",
    "|dayfirst |When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7, 2012); False by default.|\n",
    "|date_parser |Function to use to parse dates.|\n",
    "|nrows |Number of rows to read from beginning of file (not counting the header).|\n",
    "|iterator |Return a TextFileReader object for reading the file piecemeal. This object can also be used with the with statement.|\n",
    "|chunksize |For iteration, size of file chunks.|\n",
    "|skip_footer |Number of lines to ignore at end of file.|\n",
    "|verbose |Print various parsing information, like the time spent in each stage of the file conversion and memory use information.|\n",
    "|encoding |Text encoding (e.g., \"utf-8 for UTF-8 encoded text). Defaults to \"utf-8\" if None.|\n",
    "|squeeze |If the parsed data contains only one column, return a Series.|\n",
    "|thousands |Separator for thousands (e.g., \",\" or \".\"); default is None.|\n",
    "|decimal| Decimal separator in numbers (e.g., \".\" or \",\"); default is \".\".|\n",
    "|engine |CSV parsing and conversion engine to use; can be one of \"c\", \"python\", or \"pyarrow\". The default is \"c\", though the newer \"pyarrow\" engine can parse some files much faster. The \"python\" engine is slower but supports some features that the other engines do not.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
