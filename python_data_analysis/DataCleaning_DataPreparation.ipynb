{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "Data Preparation tasks such as loading, cleaning, trasforming and rearranging are ofen reported to take up 80% or more of an analyst's time. Pandas and other python features provide flexible and fast set of tools for manipulate data into the right form.\n",
    "\n",
    "Datasets: https://github.com/wesm/pydata-book/tree/3rd-edition/datasets \n",
    "\n",
    "## Index\n",
    "\n",
    "* [Handling Missing Data](#handling-missing-data)\n",
    "    * [Filtering Out Missing Data](#filtering-out-missing-data)\n",
    "    * [Filling In Missing Data](#filling-in-missing-data)\n",
    "* [Data Transformation](#data-transformation)\n",
    "    * [Removing Duplicates](#removing-duplicates)\n",
    "    * [Trasforming Data Using a Function or Mapping](#trasforming-data-using-a-function-or-mapping)\n",
    "    * [Replacing Values](#replacing-values)\n",
    "    * [Renaming Axis Indexes](#renaming-axis-indexes)\n",
    "    * [Discretization and Binning](#discretization-and-binning)\n",
    "    * [Detecting and filtering utliers](#detecting-and-filtering-outliers)\n",
    "    * [Permutation and Random Sampling](#permutation-and-random-sampling)\n",
    "    * [Computing Indicator/Dummy Variables](#computing-indicatordummy-variables)\n",
    "* [Extension Data Types](#extension-data-types)\n",
    "* [String Manipulation](#string-manipulation)\n",
    "    * [Python Built-In String Object Methods](#python-built-in-string-object-methods)\n",
    "    * [Regular Expressions](#regular-expressions)\n",
    "    * [String Functions in Pandas](#string-functions-in-pandas)\n",
    "* [Categorical Data](#categorical-data)\n",
    "    * [Background and Motivation](#background-and-motivation)\n",
    "    * [Categorical Extension Type in pandas](#categorical-extension-type-in-pandas)\n",
    "    * [Computations with Categoricals](#computations-with-categoricals)\n",
    "    * [Categorical Methods](#categorical-methods)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "*NA handling object methods*\n",
    "|Method|Description|\n",
    "|---|---|\n",
    "|dropna |Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.|\n",
    "|fillna |Fill in missing data with some value or using an interpolation method such as \"ffill\" or \"bfill\".|\n",
    "|isna |Return Boolean values indicating which values are missing/NA.|\n",
    "|notna |Negation of isna, returns True for non-NA values and False for NA values|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series \n",
      "0    1.2\n",
      "1   -3.5\n",
      "2    NaN\n",
      "3    0.0\n",
      "dtype: float64\n",
      "\n",
      "Missing data: \n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "dtype: bool\n",
      "Series \n",
      "0    1.2\n",
      "1   -3.5\n",
      "2    NaN\n",
      "3    0.0\n",
      "4    NaN\n",
      "dtype: float64\n",
      "\n",
      "Missing data: \n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "flo_data = pd.Series([1.2, -3.5, np.nan, 0])\n",
    "\n",
    "print(f\"Series \\n{flo_data}\")\n",
    "print(f\"\\nMissing data: \\n{flo_data.isna()}\")\n",
    "\n",
    "# python 'None', is also treated as NA\n",
    "flo_data = pd.Series([1.2, -3.5, np.nan, 0, None])\n",
    "\n",
    "print(f\"\\nSeries \\n{flo_data}\")\n",
    "print(f\"\\nMissing data: \\n{flo_data.isna()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Out Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series wthout NA values: \n",
      "0    1.2\n",
      "1   -3.5\n",
      "3    0.0\n",
      "dtype: float64\n",
      "\n",
      "Using filter flo_data[flo_data.notna()]: \n",
      "0    1.2\n",
      "1   -3.5\n",
      "3    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "flo_data = pd.Series([1.2, -3.5, np.nan, 0, None])\n",
    "\n",
    "print(f\"Series wthout NA values: \\n{flo_data.dropna()}\")\n",
    "# Same result with filter\n",
    "print(f\"\\nUsing filter flo_data[flo_data.notna()]: \\n{flo_data[flo_data.notna()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFrame: \n",
      "     0    1    2\n",
      "0  2.0  5.5  3.0\n",
      "1  2.0  NaN  NaN\n",
      "2  NaN  NaN  NaN\n",
      "3  NaN  5.5  3.0\n",
      "\n",
      "DFrame without rows containing NA values: \n",
      "     0    1    2\n",
      "0  2.0  5.5  3.0\n",
      "\n",
      "DFrame dropping full NA rows: \n",
      "     0    1    2\n",
      "0  2.0  5.5  3.0\n",
      "1  2.0  NaN  NaN\n",
      "3  NaN  5.5  3.0\n",
      "DFrame with new column 4 with NA values: \n",
      "     0    1    2   4\n",
      "0  2.0  5.5  3.0 NaN\n",
      "1  2.0  NaN  NaN NaN\n",
      "2  NaN  NaN  NaN NaN\n",
      "3  NaN  5.5  3.0 NaN\n",
      "\n",
      "Dropping full NA columns specifying axis:\n",
      "     0    1    2\n",
      "0  2.0  5.5  3.0\n",
      "1  2.0  NaN  NaN\n",
      "2  NaN  NaN  NaN\n",
      "3  NaN  5.5  3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "frame = pd.DataFrame([[2., 5.5, 3.], \n",
    "                      [2., np.nan, np.nan], \n",
    "                      [np.nan, np.nan, np.nan], \n",
    "                      [np.nan, 5.5, 3.]])\n",
    "\n",
    "print(f\"DFrame: \\n{frame}\")\n",
    "\n",
    "print(f\"\\nDFrame without rows containing NA values: \\n{frame.dropna()}\")\n",
    "print(f\"\\nDFrame dropping full NA rows: \\n{frame.dropna(how='all')}\")\n",
    "\n",
    "frame[4] = np.nan \n",
    "print(f\"DFrame with new column 4 with NA values: \\n{frame}\")\n",
    "print(f\"\\nDropping full NA columns specifying axis:\"\n",
    "      f\"\\n{frame.dropna(axis='columns', how='all')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "          0         1         2\n",
      "0 -0.424858       NaN       NaN\n",
      "1 -0.156521       NaN       NaN\n",
      "2 -0.475313       NaN  0.014500\n",
      "3 -0.299724       NaN -0.754694\n",
      "4 -0.068748 -0.675605  0.069436\n",
      "5 -0.223925 -0.006411  2.636000\n",
      "6 -1.614529  1.470182 -1.130516\n",
      "\n",
      "Dropping rows with NA values: \n",
      "          0         1         2\n",
      "4 -0.068748 -0.675605  0.069436\n",
      "5 -0.223925 -0.006411  2.636000\n",
      "6 -1.614529  1.470182 -1.130516\n",
      "\n",
      "Dropping rows with at least 2 NA values: \n",
      "          0         1         2\n",
      "2 -0.475313       NaN  0.014500\n",
      "3 -0.299724       NaN -0.754694\n",
      "4 -0.068748 -0.675605  0.069436\n",
      "5 -0.223925 -0.006411  2.636000\n",
      "6 -1.614529  1.470182 -1.130516\n"
     ]
    }
   ],
   "source": [
    "## Keep rows with at most a certain number of missing observations\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(np.random.standard_normal((7,3)))\n",
    "df.iloc[:4, 1] = np.nan \n",
    "df.iloc[:2, 2] = np.nan \n",
    "\n",
    "print(f\"DataFrame: \\n{df}\")\n",
    "\n",
    "print(f\"\\nDropping rows with NA values: \\n{df.dropna()}\")\n",
    "\n",
    "print(f\"\\nDropping rows with at least 2 NA values: \\n{df.dropna(thresh=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling In Missing Data\n",
    "\n",
    "Filling misssing data prevent us of discard some data with it. The `fillna()` method will help in this task. Otherwise, we can use *method='ffill'* argument with or without *limit* to fill NA values with values of the other cells in the same column. Now, argument *method='ffill'* is deprecated, instead we can use `ffill()` method or `bfill()` method.\n",
    "\n",
    "*fillna Function arguments*\n",
    "|Argument|Description|\n",
    "|---|---|\n",
    "|value |Scalar value or dictionary-like object to use to fill missing values|\n",
    "|method |Interpolation method: one of \"bfill\" (backward fill) or \"ffill\" (forward fill); default is None|\n",
    "|axis |Axis to fill on (\"index\" or \"columns\"); default is axis=\"index\"|\n",
    "|limit |For forward and backward filling, maximum number of consecutive periods to fill|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "          0         1         2\n",
      "0 -0.260455       NaN       NaN\n",
      "1 -1.361895       NaN       NaN\n",
      "2  0.485779       NaN -0.932451\n",
      "3 -0.977748       NaN  0.855924\n",
      "4  0.173131  1.399493  0.354406\n",
      "5  0.039476 -0.582290 -1.913124\n",
      "6 -0.291814  0.146749 -0.044685\n",
      "\n",
      "DataFrame filling NA with '0': \n",
      "          0         1         2\n",
      "0 -0.260455  0.000000  0.000000\n",
      "1 -1.361895  0.000000  0.000000\n",
      "2  0.485779  0.000000 -0.932451\n",
      "3 -0.977748  0.000000  0.855924\n",
      "4  0.173131  1.399493  0.354406\n",
      "5  0.039476 -0.582290 -1.913124\n",
      "6 -0.291814  0.146749 -0.044685\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(np.random.standard_normal((7,3)))\n",
    "df.iloc[:4, 1] = np.nan \n",
    "df.iloc[:2, 2] = np.nan \n",
    "\n",
    "print(f\"DataFrame: \\n{df}\")\n",
    "\n",
    "print(f\"\\nDataFrame filling NA with '0': \\n{df.fillna(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "          0         1         2\n",
      "0  0.925019  0.332396  1.077413\n",
      "1  0.360310  0.789294  0.749743\n",
      "2  0.606949       NaN -0.234555\n",
      "3 -0.404238       NaN  1.091170\n",
      "4 -2.075037       NaN       NaN\n",
      "5 -0.521436       NaN       NaN\n",
      "6 -0.551778       NaN       NaN\n",
      "\n",
      "DataFrame filling NA with values in the same column:\n",
      "          0         1         2\n",
      "0  0.925019  0.332396  1.077413\n",
      "1  0.360310  0.789294  0.749743\n",
      "2  0.606949  0.789294 -0.234555\n",
      "3 -0.404238  0.789294  1.091170\n",
      "4 -2.075037       NaN  1.091170\n",
      "5 -0.521436       NaN  1.091170\n",
      "6 -0.551778       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(np.random.standard_normal((7,3)))\n",
    "df.iloc[2:, 1] = np.nan \n",
    "df.iloc[4:, 2] = np.nan \n",
    "\n",
    "print(f\"DataFrame: \\n{df}\")\n",
    "\n",
    "#print(f\"\\nDataFrame filling NA with values in the same column:\"\n",
    "#      f\"\\n{df.fillna(method='ffill', limit=2)}\")\n",
    "\n",
    "print(f\"\\nDataFrame filling NA with values in the same column:\"\n",
    "      f\"\\n{df.ffill(limit=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Series: \n",
      "0    1.0\n",
      "1    NaN\n",
      "2    3.5\n",
      "3    NaN\n",
      "4    7.0\n",
      "dtype: float64\n",
      "\n",
      "Fillin NA with 'mean': \n",
      "0    1.000000\n",
      "1    3.833333\n",
      "2    3.500000\n",
      "3    3.833333\n",
      "4    7.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Filling data in a Series with the mean of the series\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n",
    "\n",
    "print(f\"\\nOriginal Series: \\n{data}\")\n",
    "\n",
    "print(f\"\\nFillin NA with 'mean': \\n{data.fillna(data.mean())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "### Removing Duplicates\n",
    "\n",
    "The DataFrame method `duplicated` returns a True or False whether or not the row is a duplicate (its column values are exactly eual to those in an earlier row). `drop_duplicated()` discards duplicated rows. By default `duplicated()` and `drop_duplicated()` keep the first observed value, we can change this behavior passing the argument `keep=\"last\"`, this will keep the last observerd rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "    k1  k2\n",
      "0  one   1\n",
      "1  two   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "4  one   3\n",
      "5  two   4\n",
      "6  two   4\n",
      "\n",
      "Duplicated rows: \n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "dtype: bool\n",
      "\n",
      "Dropping Duplicates: \n",
      "    k1  k2\n",
      "0  one   1\n",
      "1  two   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "4  one   3\n",
      "5  two   4\n",
      "\n",
      "Non-duplicated values based on 'K1' column:\n",
      "    k1  k2  v1\n",
      "0  one   1   0\n",
      "1  two   1   1\n",
      "\n",
      "Drop duplicates keeping last rows:\n",
      "    k1  k2  v1\n",
      "0  one   1   0\n",
      "1  two   1   1\n",
      "2  one   2   2\n",
      "3  two   3   3\n",
      "4  one   3   4\n",
      "6  two   4   6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "frame = pd.DataFrame({\n",
    "    \"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n",
    "    \"k2\": [1, 1, 2, 3, 3, 4, 4]\n",
    "})\n",
    "\n",
    "print(f\"DataFrame: \\n{frame}\")\n",
    "\n",
    "print(f\"\\nDuplicated rows: \\n{frame.duplicated()}\")\n",
    "\n",
    "# Returning DataFrame without duplicated rows \n",
    "print(f\"\\nDropping Duplicates: \\n{frame.drop_duplicates()}\")\n",
    "\n",
    "# Add column 'v1' with range(7)\n",
    "frame[\"v1\"] = range(7)\n",
    "\n",
    "# Duplicated values based only on \"k1\" column\n",
    "print(f\"\\nNon-duplicated values based on 'K1' column:\"\n",
    "      f\"\\n{frame.drop_duplicates(subset=['k1'])}\")\n",
    "\n",
    "# Keeping last observed values\n",
    "print(f\"\\nDrop duplicates keeping last rows:\"\n",
    "      f\"\\n{frame.drop_duplicates(['k1', 'k2'], keep='last')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasforming Data Using a Function or Mapping\n",
    "\n",
    "Next, we are going to add a column based on other dictionary. We'll use `map()` method which accepts a function or dictionary-like object containing a mapping to do the transformation of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "          food  ounces\n",
      "0        bacon     4.0\n",
      "1  pulled pork     3.0\n",
      "2        bacon    12.0\n",
      "3     pastrami     6.0\n",
      "4  corned beef     7.5\n",
      "5        bacon     8.0\n",
      "6     pastrami     3.0\n",
      "7    honey ham     5.0\n",
      "8     nova lox     6.0\n",
      "\n",
      "Dictionary, meat that corresponds to which animal:\n",
      "{'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow', 'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'}\n",
      "\n",
      "Updated DataFrame: \n",
      "          food  ounces  animal\n",
      "0        bacon     4.0     pig\n",
      "1  pulled pork     3.0     pig\n",
      "2        bacon    12.0     pig\n",
      "3     pastrami     6.0     cow\n",
      "4  corned beef     7.5     cow\n",
      "5        bacon     8.0     pig\n",
      "6     pastrami     3.0     cow\n",
      "7    honey ham     5.0     pig\n",
      "8     nova lox     6.0  salmon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "frame = pd.DataFrame({\n",
    "    \"food\": [\"bacon\", \"pulled pork\", \"bacon\",\n",
    "             \"pastrami\", \"corned beef\", \"bacon\",\n",
    "             \"pastrami\", \"honey ham\", \"nova lox\"],\n",
    "             \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]\n",
    "})\n",
    "\n",
    "meat_to_animal = {\n",
    "    \"bacon\": \"pig\",\n",
    "    \"pulled pork\": \"pig\",\n",
    "    \"pastrami\": \"cow\",\n",
    "    \"corned beef\": \"cow\",\n",
    "    \"honey ham\": \"pig\",\n",
    "    \"nova lox\": \"salmon\"\n",
    "}\n",
    "\n",
    "print(f\"DataFrame: \\n{frame}\")\n",
    "print(f\"\\nDictionary, meat that corresponds to which animal:\"\n",
    "      f\"\\n{meat_to_animal}\")\n",
    "\n",
    "# Creating \"animal\" column in 'frame'\n",
    "# Then, considering 'food' column we map() the 'meat_to_animal' dictionary \n",
    "frame[\"animal\"] = frame[\"food\"].map(meat_to_animal)\n",
    "\n",
    "\"\"\"\n",
    "Alternative using a function:\n",
    "def get_animal(x):\n",
    "    return meat_to_animal[x]\n",
    "\n",
    "frame[\"animal\"] = frame[\"food\"].map(get_animal)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nUpdated DataFrame: \\n{frame}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Values\n",
    "\n",
    "Sometimes, extreme values like -999 represents missing data. `replace()` can provides a simpler and more flexible way to modidy values than `map()`. The `data.replace()` method is distinct from `data.str.replace()` which performs element-wise string substitution.\n",
    "```python\n",
    "# Replace one value:\n",
    "data.replace(-999, np.nan)\n",
    "# Replace a list of values:\n",
    "data.replace([-999, -1000], np.nan)\n",
    "# Replace with diferent values:\n",
    "data.replace([-999, -1000], [np.nan, 0])\n",
    "# Replace with a dictionary:\n",
    "data.replace({-999: np.nan, -1000: 0})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Axis Indexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before changes: \n",
      "          one  two  three  four\n",
      "Ohio        0    1      2     3\n",
      "Colorado    4    5      6     7\n",
      "New York    8    9     10    11\n",
      "\n",
      "DataFrame after changes: \n",
      "      ONE  TWO  THREE  FOUR\n",
      "Ohio    0    1      2     3\n",
      "Colo    4    5      6     7\n",
      "New     8    9     10    11\n",
      "\n",
      "DataFrame after renaming: \n",
      "         one  two  peekaboo  four\n",
      "INDIANA    0    1         2     3\n",
      "COLO       4    5         6     7\n",
      "NEW        8    9        10    11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                    index=[\"Ohio\", \"Colorado\", \"New York\"],\n",
    "                    columns=[\"one\", \"two\", \"three\", \"four\"])\n",
    "\n",
    "# Function returning first 4 letters in uppercase \n",
    "def transform(x):\n",
    "    return x[:4].upper()\n",
    "\n",
    "print(f\"DataFrame before changes: \\n{data}\")\n",
    "# Modifying the DataFrame index directly\n",
    "data.index = data.index.map(transform)\n",
    "\n",
    "# And renaming as title case, no uppercase\n",
    "data2 = data.rename(index=str.title, columns=str.upper)\n",
    "\n",
    "print(f\"\\nDataFrame after changes: \\n{data2}\")\n",
    "\n",
    "# Renaming with dictionary\n",
    "data3 = data.rename(index={\"OHIO\": \"INDIANA\"},\n",
    "                    columns={\"three\": \"peekaboo\"})\n",
    "\n",
    "print(f\"\\nDataFrame after renaming: \\n{data3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning\n",
    "\n",
    "Continuous data is often discretized or otherwise separated into 'bins' for analysis. `pandas.cut()` returns a special Categorical object, each bin is identified by a special interval value type.\n",
    "\n",
    "Categories will return as: `Categories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]`, a parenthesis means that the side is open (exclusve), while the squeare bracket means it is closed (inclusive). It can be changed passing `right=false` argument: `pd.cut(ages, bins, right=False)`.\n",
    "\n",
    "`padas.cut()` with an integer number instead of explicit bin edges, it will compute equal-length bins. If you are working with decimal numbers ou can specified how many decimals will cut() consider to create bins, e.g. `pd.cut(data, 4, precision=2)` where 'precision' argument limits the decimal to two digits.\n",
    "\n",
    "Using `pandas.qcut()` bins the data on quantiles. Depending on the distribution of the data, using `pandas.cut()` will not usually result in each bin habing the same number of data ponts. But `pandas.qcut()` use sample quantiles that will give you equally sized bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 25], (18, 25], (35, 60], (60, 100], (25, 35], ..., (18, 25], (25, 35], (25, 35], (35, 60], (60, 100]]\n",
      "Length: 15\n",
      "Categories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]\n",
      "\n",
      "[0 0 2 3 1 1 2 2 2 2 0 1 1 2 3]\n",
      "\n",
      "Categories in 'age_cat': \n",
      "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], dtype='interval[int64, right]')\n",
      "\n",
      "Categorie '0' is: (18, 25]\n",
      "\n",
      "Counting values in each categorie: \n",
      "(18, 25]     3\n",
      "(25, 35]     4\n",
      "(35, 60]     6\n",
      "(60, 100]    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "ages = [20, 22, 45, 61, 27, 34, 38, 39, 41, 43, 25, 29, 31, 55, 62]\n",
    "\n",
    "# Dividing 'ages' into bins: 18-25, 26-35, 36-60, 61+\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "\n",
    "age_cat = pd.cut(ages, bins)\n",
    "\n",
    "print(f\"{age_cat}\")\n",
    "\n",
    "# Exploring new categorical valieable 'age_cat'\n",
    "print(f\"\\n{age_cat.codes}\")\n",
    "print(f\"\\nCategories in 'age_cat': \\n{age_cat.categories}\")\n",
    "print(f\"\\nCategorie '0' is: {age_cat.categories[0]}\")\n",
    "print(f\"\\nCounting values in each categorie: \\n{age_cat.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['youth', 'youth', 'middleAged', 'Senior', 'youngAdult', ..., 'youngAdult', 'youngAdult', 'youngAdult', 'middleAged', 'Senior']\n",
      "Length: 15\n",
      "Categories (4, object): ['youth' < 'youngAdult' < 'middleAged' < 'Senior']\n",
      "\n",
      "Categories in 'age_cat': \n",
      "Index(['youth', 'youngAdult', 'middleAged', 'Senior'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "ages = [20, 22, 45, 61, 27, 34, 38, 39, 41, 43, 25, 29, 31, 55, 62]\n",
    "\n",
    "# Dividing 'ages' into bins: 18-25, 26-35, 36-60, 61+\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "age_labels = [\"youth\", \"youngAdult\", \"middleAged\", \"Senior\"]\n",
    "\n",
    "age_cat = pd.cut(ages, bins, right=False, labels=age_labels)\n",
    "\n",
    "print(f\"{age_cat}\")\n",
    "print(f\"\\nCategories in 'age_cat': \\n{age_cat.categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cut: \n",
      "(-3.16, -1.55]     56\n",
      "(-1.55, 0.058]    478\n",
      "(0.058, 1.66]     415\n",
      "(1.66, 3.27]       51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data q-cut: \n",
      "(-3.1599999999999997, -0.72]    250\n",
      "(-0.72, -0.033]                 250\n",
      "(-0.033, 0.63]                  250\n",
      "(0.63, 3.27]                    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "q-cut with specific cut values: \n",
      "(-3.15, -1.248]      100\n",
      "(-1.248, -0.0334]    400\n",
      "(-0.0334, 1.269]     400\n",
      "(1.269, 3.265]       100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data = np.random.standard_normal(1000)\n",
    "\n",
    "data_cut = pd.cut(data, 4, precision=2)\n",
    "print(f\"Data cut: \\n{data_cut.value_counts()}\")\n",
    "\n",
    "data_qcut = pd.qcut(data, 4, precision=2)\n",
    "print(f\"\\nData q-cut: \\n{data_qcut.value_counts()}\")\n",
    "\n",
    "print(f\"\\nq-cut with specific cut values: \\n\"\n",
    "      f\"{pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]).value_counts()}\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Filtering Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing 'data' DataFrame: \n",
      "                 0            1            2            3\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000\n",
      "mean     -0.008834    -0.025125     0.021726    -0.011364\n",
      "std       1.010555     1.002944     0.992115     1.019340\n",
      "min      -4.569266    -4.436342    -3.466420    -3.636668\n",
      "25%      -0.713590    -0.715966    -0.625877    -0.687284\n",
      "50%       0.018050    -0.020463     0.040180    -0.027966\n",
      "75%       0.662537     0.672921     0.670434     0.666354\n",
      "max       3.811795     3.532332     3.943144     2.932167\n",
      "\n",
      "             0         1         2         3\n",
      "93    0.765138 -0.309372  3.012344  0.832574\n",
      "100   3.195022  2.122922 -0.207912 -0.720975\n",
      "118  -0.728464  0.716605  3.613930 -0.039634\n",
      "296   1.911241 -0.513293  3.379291  0.179245\n",
      "367   0.102828 -4.436342  0.572497 -1.093080\n",
      "395   1.276417 -1.877794 -0.632602 -3.636668\n",
      "428   0.561925 -3.561330  0.162459  0.530968\n",
      "485  -3.169785 -0.231187  0.018173  0.280108\n",
      "492   3.811795  0.473920 -0.812615 -0.591576\n",
      "504  -0.836411  3.532332  0.795772 -0.145170\n",
      "594   0.403325  1.003722 -0.101900 -3.599574\n",
      "730   1.027882  1.490652 -3.466420  0.342344\n",
      "899   3.138241  0.058304  0.508615  0.070832\n",
      "1008  0.117485  1.224119 -3.089392  0.392554\n",
      "1063  0.083994 -3.067637  1.223066 -0.226466\n",
      "1170 -0.036871 -3.029170 -1.540921  0.964535\n",
      "1181 -0.542447 -0.148912 -0.270349 -3.456971\n",
      "1302 -1.092013  0.130096  3.943144 -1.006752\n",
      "1332 -4.569266 -0.410851 -1.652640 -0.534387\n",
      "1436  0.218900 -3.228453 -1.116242 -0.327057\n",
      "1441 -1.432218  0.991581 -0.098594 -3.025592\n",
      "1538  0.183526  0.793838  3.176054  0.952009\n",
      "1551 -1.324002 -1.065376  0.507174 -3.577546\n",
      "1580 -0.553781 -0.523343 -1.971053 -3.573264\n",
      "1788 -0.982027 -3.317964  0.445584 -0.060545\n",
      "\n",
      "Describe data with cap: \n",
      "                 0            1            2            3\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000\n",
      "mean     -0.008537    -0.024071     0.020941    -0.009929\n",
      "std       1.005467     0.997504     0.987646     1.014719\n",
      "min      -3.000000    -3.000000    -3.000000    -3.000000\n",
      "25%      -0.713590    -0.715966    -0.625877    -0.687284\n",
      "50%       0.018050    -0.020463     0.040180    -0.027966\n",
      "75%       0.662537     0.672921     0.670434     0.666354\n",
      "max       3.000000     3.000000     3.000000     2.932167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data =  pd.DataFrame(np.random.standard_normal((2000, 4)))\n",
    "\n",
    "print(f\"Describing 'data' DataFrame: \\n{data.describe()}\")\n",
    "\n",
    "# Finding values in column 2 with values exceding 3 \n",
    "#col2 = data[2]\n",
    "#print(f\"\\nValues > 3 in col 2: \\n{col2[col2.abs() > 3]}\")\n",
    "\n",
    "# Selecting rows having a value exceeding 3 or -3\n",
    "print(f\"\\n{data[(data.abs() > 3).any(axis='columns')]}\")\n",
    "\n",
    "# To cap values outside the interval -3 to 3:\n",
    "data[data.abs() > 3 ] = np.sign(data) * 3\n",
    "# np.sign(data) produces 1 and -1 values based on positive/negative value\n",
    "\n",
    "print(f\"\\nDescribe data with cap: \\n{data.describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation and Random Sampling\n",
    "\n",
    "We can permutate (randomly reordering) with `numpy.random.permutation()` function. We generate a new variable of n numbers where n is the length of the index or number of columns. Then, we use this variable (sample in this example) to print the data with this permutation using `take()` or `iloc()`.\n",
    "\n",
    "Also, we can extract a random subset using `sample()` method. The argument `replace=True` allow us to generate a sample with replacement which allow repeat choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutating DataFrame's columns with 'take()' '[5 1 6 3 0 2 4]':\n",
      "    5   1   6   3   0   2   4\n",
      "0   5   1   6   3   0   2   4\n",
      "1  12   8  13  10   7   9  11\n",
      "2  19  15  20  17  14  16  18\n",
      "3  26  22  27  24  21  23  25\n",
      "4  33  29  34  31  28  30  32\n",
      "\n",
      "Permutating DataFrame's rows with 'iloc()' '[4 1 0 3 2]':\n",
      "    0   1   2   3   4   5   6\n",
      "4  28  29  30  31  32  33  34\n",
      "1   7   8   9  10  11  12  13\n",
      "0   0   1   2   3   4   5   6\n",
      "3  21  22  23  24  25  26  27\n",
      "2  14  15  16  17  18  19  20\n",
      "\n",
      "Random sample of 2 rows: \n",
      "    0   1   2   3   4   5   6\n",
      "4  28  29  30  31  32  33  34\n",
      "1   7   8   9  10  11  12  13\n",
      "\n",
      "Random sample of 7 number with repetition (replacement):\n",
      "4   -9\n",
      "1    5\n",
      "1    5\n",
      "4   -9\n",
      "2    7\n",
      "3   -2\n",
      "3   -2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "data = pd.DataFrame(np.arange(5 * 7).reshape((5, 7)))\n",
    "\n",
    "# Permutation of 5/7 number as data index length\n",
    "sampler5 = np.random.permutation(5)\n",
    "sampler7 = np.random.permutation(7)\n",
    "\n",
    "print(f\"\\nPermutating DataFrame's columns with 'take()' '{sampler7}':\"\n",
    "      f\"\\n{data.take(sampler7, axis='columns')}\"\n",
    "      )\n",
    "\n",
    "print(f\"\\nPermutating DataFrame's rows with 'iloc()' '{sampler5}':\"\n",
    "      f\"\\n{data.iloc[sampler5]}\"\n",
    "      )\n",
    "\n",
    "print(f\"\\nRandom sample of 2 rows: \\n{data.sample(n=2)}\")\n",
    "\n",
    "numbers = pd.Series([3, 5, 7 , -2, -9])\n",
    "print(f\"\\nRandom sample of 7 number with repetition (replacement):\"\n",
    "      f\"\\n{numbers.sample(n=7, replace=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Indicator/Dummy Variables\n",
    "\n",
    "Converting a categorical variable into a *dummy* or *indicator* matrix is usefull for statistical modeling or machine learning. Pandas has `pandas.get_dummies()` function to derive a matrix with *k* columns containing all *1s* and *0s* (True or False) corresponding to each categorie. \n",
    "\n",
    "For larger data, it would be better to write a lower-level function that writes directly to a NumPy array, and then wrap the result in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: \n",
      "  key  data1\n",
      "0   b      0\n",
      "1   b      1\n",
      "2   a      2\n",
      "3   c      3\n",
      "4   a      4\n",
      "5   b      5\n",
      "\n",
      "DataFrame dummies: \n",
      "       a      b      c\n",
      "0  False   True  False\n",
      "1  False   True  False\n",
      "2   True  False  False\n",
      "3  False  False   True\n",
      "4   True  False  False\n",
      "5  False   True  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n",
    "                   \"data1\": range(6)})\n",
    "\n",
    "print(f\"DataFrame: \\n{df}\")\n",
    "\n",
    "print(f\"\\nDataFrame dummies: \\n{pd.get_dummies(df['key'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with dummies:\n",
      "   data1  key_a  key_b  key_c\n",
      "0      0      0      1      0\n",
      "1      1      0      1      0\n",
      "2      2      1      0      0\n",
      "3      3      0      0      1\n",
      "4      4      1      0      0\n",
      "5      5      0      1      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CursosTardes\\AppData\\Local\\Temp\\ipykernel_4892\\1850580968.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_dummies = df_dummies.replace({True:1, False:0})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n",
    "                   \"data1\": range(6)})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"key\"], prefix=\"key\")\n",
    "df_dummies = df[[\"data1\"]].join(dummies)\n",
    "\n",
    "df_dummies = df_dummies.replace({True:1, False:0})\n",
    "\n",
    "print(f\"DataFrame with dummies:\\n{df_dummies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 rows from 'movies.dat' table: \n",
      "    movie_id                                  title                genres\n",
      "15        16                          Casino (1995)        Drama|Thriller\n",
      "16        17           Sense and Sensibility (1995)         Drama|Romance\n",
      "17        18                      Four Rooms (1995)              Thriller\n",
      "18        19  Ace Ventura: When Nature Calls (1995)                Comedy\n",
      "19        20                     Money Train (1995)                Action\n",
      "20        21                      Get Shorty (1995)   Action|Comedy|Drama\n",
      "21        22                         Copycat (1995)  Crime|Drama|Thriller\n",
      "22        23                       Assassins (1995)              Thriller\n",
      "23        24                          Powder (1995)          Drama|Sci-Fi\n",
      "24        25               Leaving Las Vegas (1995)         Drama|Romance\n",
      "\n",
      "Categories in the first 10 movies: \n",
      "   Action  Adventure  Animation  Children's  Comedy  Crime\n",
      "0       0          0          1           1       1      0\n",
      "1       0          1          0           1       0      0\n",
      "2       0          0          0           0       1      0\n",
      "3       0          0          0           0       1      0\n",
      "4       0          0          0           0       1      0\n",
      "5       1          0          0           0       0      1\n",
      "6       0          0          0           0       1      0\n",
      "7       0          1          0           1       0      0\n",
      "8       1          0          0           0       0      0\n",
      "9       1          1          0           0       0      0\n",
      "\n",
      "movie_id                                       1\n",
      "title                           Toy Story (1995)\n",
      "genres               Animation|Children's|Comedy\n",
      "Genre_Action                                   0\n",
      "Genre_Adventure                                0\n",
      "Genre_Animation                                1\n",
      "Genre_Children's                               1\n",
      "Genre_Comedy                                   1\n",
      "Genre_Crime                                    0\n",
      "Genre_Documentary                              0\n",
      "Genre_Drama                                    0\n",
      "Genre_Fantasy                                  0\n",
      "Genre_Film-Noir                                0\n",
      "Genre_Horror                                   0\n",
      "Genre_Musical                                  0\n",
      "Genre_Mystery                                  0\n",
      "Genre_Romance                                  0\n",
      "Genre_Sci-Fi                                   0\n",
      "Genre_Thriller                                 0\n",
      "Genre_War                                      0\n",
      "Genre_Western                                  0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "mnames = [\"movie_id\", \"title\", \"genres\"]\n",
    "movies = pd.read_table(\"datasets/movies.dat\", sep=\"::\", \n",
    "                       header=None, names=mnames, engine=\"python\")\n",
    "\n",
    "print(f\"10 rows from 'movies.dat' table: \\n{movies[15:25]}\")\n",
    "\n",
    "m_dummies = movies[\"genres\"].str.get_dummies(\"|\")\n",
    "print(f\"\\nCategories in the first 10 movies: \\n{m_dummies.iloc[:10, :6]}\")\n",
    "\n",
    "movies_windic = movies.join(m_dummies.add_prefix(\"Genre_\"))\n",
    "print(f\"\\n{movies_windic.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of values: \n",
      "[0.85583096 0.38204763 0.4648361  0.57742467 0.94388731 0.22337882\n",
      " 0.21534198 0.56622647 0.72537081 0.64196748]\n",
      "\n",
      "Categorized Values: \n",
      "   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]\n",
      "0       False       False       False       False        True\n",
      "1       False        True       False       False       False\n",
      "2       False       False        True       False       False\n",
      "3       False       False        True       False       False\n",
      "4       False       False       False       False        True\n",
      "5       False        True       False       False       False\n",
      "6       False        True       False       False       False\n",
      "7       False       False        True       False       False\n",
      "8       False       False       False        True       False\n",
      "9       False       False       False        True       False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "np.random.seed(255198)\n",
    "\n",
    "values = np.random.uniform(size=10)\n",
    "\n",
    "print(f\"Array of values: \\n{values}\")\n",
    "\n",
    "# Combining pandas.get_dummies() with pandas.cut()\n",
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "cat_values = pd.get_dummies(pd.cut(values, bins))\n",
    "\n",
    "print(f\"\\nCategorized Values: \\n{cat_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Data Types\n",
    "\n",
    "*Pandas extension data types*\n",
    "|Extension type|Description|\n",
    "|---|---|\n",
    "|BooleanDtype |Nullable Boolean data, use \"boolean\" when passing as string|\n",
    "|CategoricalDtype |Categorical data type, use \"category\" when passing as string|\n",
    "|DatetimeTZDtype |Datetime with time zone|\n",
    "|Float32Dtype |32-bit nullable floating point, use \"Float32\" when passing as string|\n",
    "|Float64Dtype |64-bit nullable floating point, use \"Float64\" when passing as string|\n",
    "|Int8Dtype| 8-bit nullable signed integer, use \"Int8\" when passing as string|\n",
    "|Int16Dtype |16-bit nullable signed integer, use \"Int16\" when passing as string|\n",
    "|Int32Dtype |32-bit nullable signed integer, use \"Int32\" when passing as string|\n",
    "|Int64Dtype |64-bit nullable signed integer, use \"Int64\" when passing as string|\n",
    "|UInt8Dtype |8-bit nullable unsigned integer, use \"UInt8\" when passing as string|\n",
    "|UInt16Dtype |16-bit nullable unsigned integer, use \"UInt16\" when passing as string|\n",
    "|UInt32Dtype |32-bit nullable unsigned integer, use \"UInt32\" when passing as string|\n",
    "|UInt64Dtype |64-bit nullable unsigned integer, use \"UInt64\" when passing as string|\n",
    "\n",
    "Extension types can be passed to the Series `astype()` method: `df[\"A\"] = df[\"A\"].astype(\"Float64\")`.\n",
    "\n",
    "On large datasets, string arrays like: `pd.Series(['one', 'two', None, 'three'], dtype=pd.StringDtype())` use much less memory and are more efficient.\n",
    "\n",
    "Using pandas extension data type, e.g. pd.Int64Dtype, missing values will be pd.NA instead of np.nan: `pd.Series([1, 2, 3, None], dtype=pd.Int64Dtype())`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation\n",
    "\n",
    "### Python Built-In String Object Methods\n",
    "\n",
    "*Python built-in string methods*\n",
    "|Method|Description|\n",
    "|---|---|\n",
    "|count |Return the number of nonoverlapping occurrences of substring in the string|\n",
    "|endswith |Return True if string ends with suffix|\n",
    "|startswith |Return True if string starts with prefix|\n",
    "|join |Use string as delimiter for concatenating a sequence of other strings|\n",
    "|index |Return starting index of the first occurrence of passed substring if found in the string; otherwise, raises ValueError if not found|\n",
    "|find |Return position of first character of first occurrence of substring in the string; like index, but returns –1 if not found|\n",
    "|rfind |Return position of first character of last occurrence of substring in the string; returns –1 if not found|\n",
    "|replace |Replace occurrences of string with another string|\n",
    "|strip, rstrip, lstrip |Trim whitespace, including newlines on both sides, on the right side, or on the left side, respectively|\n",
    "|split |Break string into list of substrings using passed delimiter|\n",
    "|lower |Convert alphabet characters to lowercase|\n",
    "|upper |Convert alphabet characters to uppercase|\n",
    "|casefold |Convert characters to lowercase, and convert any region-specific variable character combinations to a common comparable form|\n",
    "|ljust, rjust |Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill character) to return a string with a minimum width|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: ['a', 'b', 'c', 'delta']\n",
      "Values with delimeter: a::b::c::delta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "values = \"a,b, c,   delta\"\n",
    "\n",
    "# combining split with strip to separate values and remove whitespaces \n",
    "pieces = [x.strip() for x in values.split(\",\")]\n",
    "print(f\"Values: {pieces}\")\n",
    "\n",
    "# Adding two-colon delimeter with 'join'\n",
    "pieces = \"::\".join(pieces)\n",
    "print(f\"Values with delimeter: {pieces}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "|Method|Description|\n",
    "|---|---|\n",
    "|findall |Return all nonoverlapping matching patterns in a string as a list|\n",
    "|finditer |Like findall, but returns an iterator|\n",
    "|match |Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, return a match object, and otherwise None|\n",
    "|search |Scan string for match to pattern, returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning|\n",
    "|split |Break string into pieces at each occurrence of pattern|\n",
    "|sub, subn |Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols \\1, \\2, ... to refer to match group elements in the replacement string|\n",
    "\n",
    "Creating a 'regex' object with `re.compile` will save CPU cycles if you intend to apply the same expression to many strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 'text' clean:\n",
      "['foo', 'bar', 'baz', 'qux']\n",
      "\n",
      "Finding emails: \n",
      "['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']\n",
      "\n",
      "Finding emails: \n",
      "[('dave', 'google', 'com'), ('steve', 'gmail', 'com'), ('rob', 'gmail', 'com'), ('ryan', 'yahoo', 'com')]\n",
      "\n",
      "    Dave Username: dave, Domain: google, Suffix: com\n",
      "    Steve Username: steve, Domain: gmail, Suffix: com\n",
      "    Rob Username: rob, Domain: gmail, Suffix: com\n",
      "    Ryan Username: ryan, Domain: yahoo, Suffix: com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re # regular expresions\n",
    "\n",
    "regex_space = re.compile(r\"\\s+\") # Regular expresion looking for whitespaces\n",
    "\n",
    "text = \"foo bar\\t  baz    \\tqux\"\n",
    "\n",
    "print(f\"Printing 'text' clean:\\n{regex_space.split(text)}\")\n",
    "\n",
    "text_mail = \"\"\"\n",
    "    Dave dave@google.com\n",
    "    Steve steve@gmail.com\n",
    "    Rob rob@gmail.com\n",
    "    Ryan ryan@yahoo.com\n",
    "\"\"\"\n",
    "\n",
    "patt_mail = r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\"\n",
    "patt_mail_seg = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
    "\n",
    "\n",
    "# extracting emails with previous pattern\n",
    "# using 're.INGORECASE' to make regex case insensitive\n",
    "regex_mail = re.compile(patt_mail, flags=re.IGNORECASE)\n",
    "\n",
    "regex_mail_seg = re.compile(patt_mail_seg, flags=re.IGNORECASE)\n",
    "\n",
    "print(f\"\\nFinding emails: \\n{regex_mail.findall(text_mail)}\")\n",
    "\n",
    "print(f\"\\nFinding emails: \\n{regex_mail_seg.findall(text_mail)}\")\n",
    "\n",
    "print(regex_mail_seg.sub(r\"Username: \\1, Domain: \\2, Suffix: \\3\", text_mail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Functions in Pandas\n",
    "\n",
    "*Partial listing of Series string methods*\n",
    "|Method|Description|\n",
    "|---|---|\n",
    "|cat |Concatenate strings element-wise with optional delimiter|\n",
    "|contains |Return Boolean array if each string contains pattern/regex|\n",
    "|count |Count occurrences of pattern|\n",
    "|extract |Use a regular expression with groups to extract one or more strings from a Series of strings; the result will be a DataFrame with one column per group|\n",
    "|endswith |Equivalent to x.endswith(pattern) for each element|\n",
    "|startswith |Equivalent to x.startswith(pattern) for each element|\n",
    "|findall |Compute list of all occurrences of pattern/regex for each string|\n",
    "|get| Index into each element (retrieve i-th element)|\n",
    "|isalnum |Equivalent to built-in str.alnum|\n",
    "|isalpha| Equivalent to built-in str.isalpha|\n",
    "|isdecimal| Equivalent to built-in str.isdecimal|\n",
    "|isdigit| Equivalent to built-in str.isdigit|\n",
    "|islower |Equivalent to built-in str.islower|\n",
    "|isnumeric |Equivalent to built-in str.isnumeric|\n",
    "|isupper |Equivalent to built-in str.isupper|\n",
    "|join |Join strings in each element of the Series with passed separator|\n",
    "|len |Compute length of each string|\n",
    "|lower, upper |Convert cases; equivalent to x.lower() or x.upper() for each element|\n",
    "|match |Use re.match with the passed regular expression on each element, returning True or False whether it matches|\n",
    "|pad |Add whitespace to left, right, or both sides of strings|\n",
    "|center |Equivalent to pad(side=\"both\")|\n",
    "|repeat| Duplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string)|\n",
    "|replace |Replace occurrences of pattern/regex with some other string|\n",
    "|slice |Slice each string in the Series|\n",
    "|split |Split strings on delimiter or regular expression|\n",
    "|strip |Trim whitespace from both sides, including newlines|\n",
    "|rstrip |Trim whitespace on right side|\n",
    "|lstrip |Trim whitespace on left side|\n",
    "\n",
    "Regular expressions can be passed using data.map but will fail with NA values. In this case we can use Seres methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      "Dave     dave@google.com\n",
      "Steve    steve@gmail.com\n",
      "Rob        rob@gmail.com\n",
      "Wes                  NaN\n",
      "dtype: object\n",
      "\n",
      "NA values in data: \n",
      "Dave     False\n",
      "Steve    False\n",
      "Rob      False\n",
      "Wes       True\n",
      "dtype: bool\n",
      "\n",
      "Data as string Dtype: \n",
      "Dave     dave@google.com\n",
      "Steve    steve@gmail.com\n",
      "Rob        rob@gmail.com\n",
      "Wes                 <NA>\n",
      "dtype: string\n",
      "\n",
      "Dave     (dave, google, com)\n",
      "Steve    (steve, gmail, com)\n",
      "Rob        (rob, gmail, com)\n",
      "Wes                      NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "data = {\"Dave\": \"dave@google.com\", \"Steve\": \"steve@gmail.com\",\n",
    "        \"Rob\": \"rob@gmail.com\", \"Wes\": np.nan}\n",
    "\n",
    "data = pd.Series(data)\n",
    "\n",
    "print(f\"Data: \\n{data}\")\n",
    "print(f\"\\nNA values in data: \\n{data.isna()}\")\n",
    "\n",
    "data_string = data.astype(\"string\")\n",
    "\n",
    "print(f\"\\nData as string Dtype: \\n{data_string}\")\n",
    "\n",
    "patt_mail = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
    "\n",
    "match_mail = data.str.findall(patt_mail, flags=re.IGNORECASE).str[0]\n",
    "\n",
    "print(f\"\\n{match_mail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "### Background and Motivation\n",
    "\n",
    "*Categories* are the array of distincst values, and the integer values that reference the categories are *category codes*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series values: \n",
      "0     apple\n",
      "1    orange\n",
      "2     apple\n",
      "3     apple\n",
      "4     apple\n",
      "5    orange\n",
      "6     apple\n",
      "7     apple\n",
      "dtype: object\n",
      "\n",
      "Unique values in values: ['apple' 'orange']\n",
      "\n",
      "Value counts: \n",
      "apple     6\n",
      "orange    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "values = pd.Series([\"apple\", \"orange\", \"apple\", \"apple\"] * 2)\n",
    "\n",
    "print(f\"Series values: \\n{values}\")\n",
    "\n",
    "print(f\"\\nUnique values in values: {pd.unique(values)}\")\n",
    "\n",
    "print(f\"\\nValue counts: \\n{values.value_counts()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category codes: \n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "6    0\n",
      "7    0\n",
      "dtype: int64\n",
      "\n",
      "Categories: \n",
      "0     apple\n",
      "1    orange\n",
      "dtype: object\n",
      "\n",
      "Categories instead of codes: \n",
      "0     apple\n",
      "1    orange\n",
      "0     apple\n",
      "0     apple\n",
      "0     apple\n",
      "1    orange\n",
      "0     apple\n",
      "0     apple\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# categories and category codes\n",
    "codes = pd.Series([0, 1, 0, 0] * 2)\n",
    "cats = pd.Series([\"apple\", \"orange\"])\n",
    "\n",
    "print(f\"Category codes: \\n{codes}\")\n",
    "print(f\"\\nCategories: \\n{cats}\")\n",
    "\n",
    "# 'take()' method to replace codes\n",
    "print(f\"\\nCategories instead of codes: \\n{cats.take(codes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Extension Type in pandas\n",
    "\n",
    "A popular data compresion technique is *encoding* (integer-based categorical representation) for data with many ocurrences of similar values. It can provide faster performance and lower memory use.\n",
    "\n",
    "We can convert into categories if we have enconded data with `pandas.Categorical.from_codes()` passing *codes, categories* arguments as lists. Unless explicitly specified, categorical conversions assume no specific ordering. Using `from_codes()` or other constructor, you can indicate that the categories hava a meaningful ordering `pandas.Categorical.from_codes(codes, categories, ordered=True)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruits DataFrame: \n",
      "   basket_id   fruit  count    weight\n",
      "0          0   apple      3  1.019423\n",
      "1          1  orange      4  3.662935\n",
      "2          2   apple      3  2.714591\n",
      "3          3   apple      3  3.616318\n",
      "4          4   apple      3  3.535980\n",
      "5          5  orange      3  2.854480\n",
      "6          6   apple      4  0.004824\n",
      "7          7   apple      4  1.031763\n",
      "\n",
      "Type of fruit categories: ['apple', 'orange', 'apple', 'apple', 'apple', 'orange', 'apple', 'apple']\n",
      "Categories (2, object): ['apple', 'orange']\n",
      "\n",
      "Fruit categories: Index(['apple', 'orange'], dtype='object')\n",
      "Fruit codes: [0 1 0 0 0 1 0 0]\n",
      "\n",
      "Codes and categories: {0: 'apple', 1: 'orange'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "fruits = [\"apple\", \"orange\", \"apple\", \"apple\"] * 2\n",
    "\n",
    "N = len(fruits)\n",
    "\n",
    "rng = np.random.default_rng(seed=255198)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"fruit\": fruits,\n",
    "    \"basket_id\": np.arange(N),\n",
    "    \"count\": rng.integers(3, 5, size=N),\n",
    "    \"weight\": rng.uniform(0, 4, size=N)\n",
    "    },\n",
    "    columns=[\"basket_id\", \"fruit\", \"count\", \"weight\"]\n",
    "    )\n",
    "\n",
    "print(f\"Fruits DataFrame: \\n{df}\")\n",
    "\n",
    "# Converting 'froit' column into categorical data type\n",
    "fruit_cat = df[\"fruit\"].astype(\"category\")\n",
    "# now 'fruit_cat' is an instance of 'pandas.Categorical' \n",
    "fc = fruit_cat.array \n",
    "print(f\"\\nType of fruit categories: {fc}\")\n",
    "print(f\"\\nFruit categories: {fc.categories}\")\n",
    "print(f\"Fruit codes: {fc.codes}\")\n",
    "\n",
    "## Mapping between codes and categories:\n",
    "print(f\"\\nCodes and categories: {dict(enumerate(fc.categories))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My categories: \n",
      "['miu', 'mia', 'fiu', 'mia', 'miu']\n",
      "Categories (3, object): ['fiu', 'mia', 'miu']\n",
      "\n",
      "My alternative categories: ['foo', 'bar', 'baz', 'foo', 'foo', 'bar']\n",
      "Categories (3, object): ['foo', 'bar', 'baz']\n",
      "\n",
      "Ordering my alt-cat: \n",
      "['foo', 'bar', 'baz', 'foo', 'foo', 'bar']\n",
      "Categories (3, object): ['foo' < 'bar' < 'baz']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "## Creating directly a categorical object\n",
    "my_cats = pd.Categorical([\"miu\", \"mia\", \"fiu\", \"mia\", \"miu\"])\n",
    "\n",
    "print(f\"My categories: \\n{my_cats}\")\n",
    "\n",
    "## Alternative constructor:\n",
    "categories = [\"foo\", \"bar\", \"baz\"]\n",
    "codes = [0, 1, 2, 0, 0, 1]\n",
    "\n",
    "my_cats_2 = pd.Categorical.from_codes(codes, categories)\n",
    "print(f\"\\nMy alternative categories: {my_cats_2}\")\n",
    "print(f\"\\nOrdering my alt-cat: \\n{my_cats_2.as_ordered()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computations with Categoricals\n",
    "\n",
    "Some parts of pandas performs better working with categoricals like the *groupby* function. `pandas.qcut()` returns pandas.Categorical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in bins: \n",
      "['Q1', 'Q1', 'Q4', 'Q2', 'Q2', ..., 'Q2', 'Q3', 'Q4', 'Q3', 'Q1']\n",
      "Length: 1000\n",
      "Categories (4, object): ['Q1' < 'Q2' < 'Q3' < 'Q4']\n",
      "\n",
      "Codes: [0 0 3 1 1 0 1 0 3 0 2 2 2 0 2]\n",
      "\n",
      "Results: \n",
      "  quartile  count       min       max\n",
      "0       Q1    250 -2.916361 -0.576712\n",
      "1       Q2    250 -0.575496  0.017787\n",
      "2       Q3    250  0.023344  0.724804\n",
      "3       Q4    250  0.726007  2.804314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CursosTardes\\AppData\\Local\\Temp\\ipykernel_17556\\4004861786.py:15: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(bins)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "rng = np.random.default_rng(seed=255186)\n",
    "draws = rng.standard_normal(1000)\n",
    "\n",
    "# Generating 'bins' with 'qcut' and labeling \n",
    "bins = pd.qcut(draws, 4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "print(f\"Data in bins: \\n{bins}\")\n",
    "print(f\"\\nCodes: {bins.codes[:15]}\")\n",
    "\n",
    "bins = pd.Series(bins, name=\"quartile\")\n",
    "results = (pd.Series(draws)\n",
    "           .groupby(bins)\n",
    "           .agg([\"count\", \"min\", \"max\"])\n",
    "           .reset_index())\n",
    "\n",
    "print(f\"\\nResults: \\n{results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better performance with categoricals\n",
    "\n",
    "If we have a large number of entries with some categories as labels (string), we can convert this labels (e.g. pandas.Series) to a 'category' Dtype with `.astype()` method. In comparison, the default labels use significantly more memory than 'categories' Dtype. Also, groupby operations can be faster because the algorithm it use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Methods\n",
    "\n",
    "*Categorical method for Series in pandas*\n",
    "|Method|Description|\n",
    "|---|---|\n",
    "|add_categories |Append new (unused) categories at end of existing categories|\n",
    "|as_ordered |Make categories ordered|\n",
    "|as_unordered |Make categories unordered|\n",
    "|remove_categories |Remove categories, setting any removed values to null|\n",
    "|remove_unused_categories |Remove any category values that do not appear in the data|\n",
    "|rename_categories |Replace categories with indicated set of new category names; cannot change the number of categories|\n",
    "|reorder_categories |Behaves like rename_categories, but can also change the result to have ordered categories|\n",
    "|set_categories |Replace the categories with the indicated set of new categories; can add or remove categories|\n",
    "\n",
    "With pandas series containing categorical Dtype, the special *accessor* attribute `cat` provides access to categorical methods.\n",
    "\n",
    "Categoricals are used as a convenient tool for memory savings and better performance, but in large datasets after filter the DataFrame or Series, many categories may not appear. We can use `remove_unused_categories` method to trim unobserved categories.\n",
    "\n",
    "**Creating dummy variables for modeling** with `pandas.get_dummies` function which converts one dimensional categorical data into a DataFrame (1s for occurrences of a given category and 0s otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series codes:\n",
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    0\n",
      "5    1\n",
      "6    2\n",
      "7    3\n",
      "dtype: int8\n",
      "\n",
      "Actual categories: \n",
      "0    a\n",
      "1    b\n",
      "2    c\n",
      "3    d\n",
      "4    a\n",
      "5    b\n",
      "6    c\n",
      "7    d\n",
      "dtype: category\n",
      "Categories (5, object): ['a', 'b', 'c', 'd', 'e']\n",
      "\n",
      "Count actual categories: \n",
      "a    2\n",
      "b    2\n",
      "c    2\n",
      "d    2\n",
      "e    0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categories from series 3: Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "\n",
      "Removing unused categories: \n",
      "0    a\n",
      "1    b\n",
      "4    a\n",
      "5    b\n",
      "dtype: category\n",
      "Categories (2, object): ['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "ser = pd.Series([\"a\", \"b\", \"c\", \"d\"] * 2)\n",
    "cat_ser = ser.astype(\"category\")\n",
    "\n",
    "print(f\"Series codes:\\n{cat_ser.cat.codes}\")\n",
    "\n",
    "act_cat = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "\n",
    "# Series with new 'actual' categories\n",
    "cat_ser2 = cat_ser.cat.set_categories(act_cat)\n",
    "print(f\"\\nActual categories: \\n{cat_ser2}\")\n",
    "print(f\"\\nCount actual categories: \\n{cat_ser2.value_counts()}\")\n",
    "\n",
    "cat_ser3 = cat_ser[cat_ser.isin(['a', 'b'])]\n",
    "print(f\"\\nCategories from series 3: {cat_ser3.cat.categories}\")\n",
    "print(f\"\\nRemoving unused categories: \\n{cat_ser3.cat.remove_unused_categories()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
