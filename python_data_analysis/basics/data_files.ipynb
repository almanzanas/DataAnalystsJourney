{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Storage, and File Formats\n",
    "\n",
    "*Data loading* term refers to read data and making it accessible. *Parsing* is also used to describe loading text data and interpreting it as tables and different data types.\n",
    "\n",
    "Resources: https://github.com/wesm/pydata-book \n",
    "\n",
    "## Index\n",
    "\n",
    "* [Reading and Writing Data in Text Format](#reading-and-writing-data-in-text-format)\n",
    "    * [pandas.read_csv Functions](#some-pandasread_csv-function-arguments)\n",
    "    * [Reading Text Files in Pieces](#reading-text-files-in-pieces)\n",
    "    * [Writing Data to Text Format](#writing-data-to-text-format)\n",
    "    * [Working with Other Delimited Formats](#working-with-other-delimited-formats)\n",
    "    * [JSON data](#json-data)\n",
    "    * [XML and HTML: Web Scraping](#xml-and-html-web-scraping)\n",
    "* [Binary Data Formats](#binary-data-formats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data in Text Format\n",
    "\n",
    "*Text and binary data loading functions in pandas*\n",
    "|Function|Description|\n",
    "|---|---|\n",
    "|**read_csv** |**Load delimited data from a file, URL, or file-like object; use comma as default delimiter**|\n",
    "|read_fwf |Read data in fixed-width column format (i.e., no delimiters)|\n",
    "|read_clipboard |Variation of read_csv that reads data from the clipboard; useful for converting tables from web pages|\n",
    "|read_excel |Read tabular data from an Excel XLS or XLSX file|\n",
    "|read_hdf |Read HDF5 files written by pandas|\n",
    "|read_html |Read all tables found in the given HTML document|\n",
    "|**read_json** |**Read data from a JSON (JavaScript Object Notation) string representation, file, URL, or file-like object**|\n",
    "|read_feather |Read the Feather binary file format|\n",
    "|read_orc |Read the Apache ORC binary file format|\n",
    "|read_parquet |Read the Apache Parquet binary file format|\n",
    "|read_pickle |Read an object stored by pandas using the Python pickle format|\n",
    "|read_sas |Read a SAS dataset stored in one of the SAS system’s custom storage formats|\n",
    "|**read_spss** |Read a data file created by SPSS|\n",
    "|read_sql |Read the results of a SQL query (using SQLAlchemy)|\n",
    "|read_sql_table |Read a whole SQL table (using SQLAlchemy); equivalent to using a query that selects everything in that table using read_sql|\n",
    "|read_stata |Read a dataset from Stata file format|\n",
    "|read_xml |Read a table of data from an XML file|\n",
    "\n",
    "Some of this functions has a long list of optional arguments, `pandas.read_csv()` has around 50, so ig you are struggling to read a particular file you can look online to found your optimal arguments.\n",
    "\n",
    "```python\n",
    "# some examples:\n",
    "\n",
    "# The csv file has not headers, it will read with default names\n",
    "pd.read_csv(\"example.csv\", header=None)\n",
    "# or you can specify names\n",
    "pd.read_csv(\"example.csv\", names[\"a\", \"b\", \"c\", \"d\", \"message\"],\n",
    "            index_col=\"message\")\n",
    "# the argument 'index_col=\"message\"' to indicate your index column\n",
    "\n",
    "# Pass multiple col names (list) for a hierarchical index\n",
    "\n",
    "# You can pass a regular expression as delimeter for pandas\n",
    "# use sep=\"\\s+\" if the file are separated for a variable amount of whitespaces\n",
    "\n",
    "# with skiprows=[2, 3, 5] you can skip that rows\n",
    "pd.read_csv(\"example.csv\", skiprows=[2, 3, 5])\n",
    "```\n",
    "\n",
    "#### *Some pandas.read_csv function arguments*\n",
    "|Argument|Description|\n",
    "|---|---|\n",
    "|path |String indicating filesystem location, URL, or file-like object.|\n",
    "|sep or delimiter |Character sequence or regular expression to use to split fields in each row.|\n",
    "|header |Row number to use as column names; defaults to 0 (first row), but should be None if there is no header row.|\n",
    "|index_col |Column numbers or names to use as the row index in the result; can be a single name/number or a list of them for a hierarchical index.|\n",
    "|names |List of column names for result.|\n",
    "|skiprows| Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip.|\n",
    "|na_values |Sequence of values to replace with NA. They are added to the default list unless keep_default_na=False is passed.|\n",
    "|keep_default_na |Whether to use the default NA value list or not (True by default).|\n",
    "|comment| Character(s) to split comments off the end of lines.|\n",
    "|parse_dates |Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise, can specify a list of column numbers or names to parse. If element of list is tuple or list, will combine multiple columns together and parse to date (e.g., if date/time split across two columns).|\n",
    "|keep_date_col |If joining columns to parse date, keep the joined columns; False by default.|\n",
    "|converters |Dictionary containing column number or name mapping to functions (e.g., {\"foo\": f} would apply the function f to all values in the \"foo\" column).|\n",
    "|dayfirst |When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7, 2012); False by default.|\n",
    "|date_parser |Function to use to parse dates.|\n",
    "|nrows |Number of rows to read from beginning of file (not counting the header).|\n",
    "|iterator |Return a TextFileReader object for reading the file piecemeal. This object can also be used with the with statement.|\n",
    "|chunksize |For iteration, size of file chunks.|\n",
    "|skip_footer |Number of lines to ignore at end of file.|\n",
    "|verbose |Print various parsing information, like the time spent in each stage of the file conversion and memory use information.|\n",
    "|encoding |Text encoding (e.g., \"utf-8 for UTF-8 encoded text). Defaults to \"utf-8\" if None.|\n",
    "|squeeze |If the parsed data contains only one column, return a Series.|\n",
    "|thousands |Separator for thousands (e.g., \",\" or \".\"); default is None.|\n",
    "|decimal| Decimal separator in numbers (e.g., \".\" or \",\"); default is \".\".|\n",
    "|engine |CSV parsing and conversion engine to use; can be one of \"c\", \"python\", or \"pyarrow\". The default is \"c\", though the newer \"pyarrow\" engine can parse some files much faster. The \"python\" engine is slower but supports some features that the other engines do not.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces\n",
    "\n",
    "Processing a large files you may want to read a small piece or it. But before you can limit pandas display: `pd.option.display.max_rows=10`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        one       two     three      four key\n",
      "0  0.467976 -0.038649 -0.295344 -1.824726   L\n",
      "1 -0.358893  1.404453  0.704965 -0.200638   B\n",
      "2 -0.501840  0.659254 -0.421691 -0.057688   G\n",
      "3  0.204886  1.074134  1.388361 -0.982404   R\n",
      "4  0.354628 -0.133116  0.283763 -0.837063   Q\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# limit the number of rows you want to see\n",
    "result = pd.read_csv(\"../datasets/ex6.csv\", nrows=5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key\n",
      "E    368.0\n",
      "X    364.0\n",
      "L    346.0\n",
      "O    343.0\n",
      "Q    340.0\n",
      "M    338.0\n",
      "J    337.0\n",
      "F    335.0\n",
      "K    334.0\n",
      "H    330.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# to read a file in pieces, chunksize as a number of rows\n",
    "chunker = pd.read_csv(\"../datasets/ex6.csv\", chunksize=1000)\n",
    "\n",
    "# Creating empty Series\n",
    "tot = pd.Series([], dtype='int64')\n",
    "\n",
    "# Exploring 'chunker' and adding key counts to the series\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "\n",
    "tot = tot.sort_values(ascending=False)\n",
    "\n",
    "print(tot[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Text Format\n",
    "\n",
    "```python\n",
    "# Variable with data:\n",
    "data = pd.read_csv(\"example.csv\")\n",
    "\n",
    "# Exporting to csv:\n",
    "data.to_csv(\"example_out.csv\")\n",
    "\n",
    "# to write in the CSV you want to export, you'll need 'import sys'\n",
    "# Changing delimiters:\n",
    "data.to_csv(sys.stdout, sep=\"|\")\n",
    "\n",
    "# Filling NA values with string:\n",
    "data.to_csv(sys,stdout, na_rep=\"NULL\")\n",
    "\n",
    "# By dafault, to_csv() write column and row labels. You can disabled with:\n",
    "data.to_csv(sys.stdout, index=False, header=False)\n",
    "\n",
    "# Choosing columns and it's order:\n",
    "data.to_csv(sys.stdout, index=False, columns=[\"a\". \"b\", \"c\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Other Delimited Formats\n",
    "\n",
    "In some cases, `pandas.read_csv()` will not work because the file CSV has malformed lines. In this cases you will need manual processing for the CSV file with `import csv` and *for* loop.\n",
    "\n",
    "*CSV dialect options*\n",
    "|Argument|Description|\n",
    "|---|---|\n",
    "|delimiter |One-character string to separate fields; defaults to \",\".|\n",
    "|lineterminator |Line terminator for writing; defaults to \"\\r\\n\". Reader ignores this and recognizes cross-platform line terminators.|\n",
    "|quotechar |Quote character for fields with special characters (like a delimiter); default is '\"'.|\n",
    "|quoting |Quoting convention. Options include csv.QUOTE_ALL (quote all fields), csv.QUOTE_MINI MAL (only fields with special characters like the delimiter), csv. UOTE_NONNUMERIC, and csv.QUOTE_NONE (no quoting). See Python’s documentation for full details. Defaults to QUOTE_MINIMAL.|\n",
    "|skipinitialspace |Ignore whitespace after each delimiter; default is False.|\n",
    "|doublequote |How to handle quoting character inside a field; if True, it is doubled (see online documentation for full detail and behavior).|\n",
    "|escapechar |String to escape the delimiter if quoting is set to csv.QUOTE_NONE; disabled by default.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['1', '2', '3']\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "# Opening and reading CSV file\n",
    "file = open(\"../datasets/ex7.csv\")\n",
    "reader = csv.reader(file)\n",
    "\n",
    "# iterating line by line with 'for' loop \n",
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c', 'd'], ['1', '2', '3', '4'], ['1', '2', '3', '4'], ['1', '2', '3', '4'], ['1', '2', '3', '4'], ['1', '2', '3', '4'], ['1', '2', '3', '4']]\n",
      "New data: \n",
      "{'a': ('1', '1', '1', '1', '1', '1'), 'b': ('2', '2', '2', '2', '2', '2'), 'c': ('3', '3', '3', '3', '3', '3'), 'd': ('4', '4', '4', '4', '4', '4')}\n"
     ]
    }
   ],
   "source": [
    "### Putting the data in needed form\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"../datasets/ex7.csv\") as file:\n",
    "    lines = list(csv.reader(file))\n",
    "\n",
    "# Selecting Header and Values:\n",
    "header, values = lines[0], lines[1:]\n",
    "\n",
    "# Dictionary comprehension\n",
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "\"\"\"\n",
    "    in larger files use a lot of memory\n",
    "    'h' and 'v' represents header and values\n",
    "\"\"\"\n",
    "\n",
    "print(f\"New data: \\n{data_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Data\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Reading code written in the python file (python object)\n",
    "data = {...} \n",
    "result = json.loads(data)\n",
    "# Convert json to python object\n",
    "asjson = json.dumps(result)\n",
    "\n",
    "import pandas as pd \n",
    "# Converting JSON datasets into Series or DataFrame\n",
    "df = pd.read_json(\"example.json\")\n",
    "\n",
    "# Nested JSON:\n",
    "with open(\"example.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "content = data[\"companies\"]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for item in content:\n",
    "    temp_df = pd.json_normalize(item, record_path=[\"employees\"], meta=[\"company\"])\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    \n",
    "print(df)\n",
    "\n",
    "# Export data from pandas to JSON, there are two formats:\n",
    "df.to_json(sys.stdout)\n",
    "df.to_json(sys.stdout, orient=\"records\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML and HTML: Web Scraping\n",
    "\n",
    "There are many python libraries for read html and xml (lxml, Beautiful Soup, html5lib...), and padas has `pandas.read_html` to parse tables out of HTML files as DataFrame objects. We will need some additional libraries:\n",
    "\n",
    "```python\n",
    "conda install lxml beautifulsoup4 html5lib\n",
    "# or\n",
    "pip install lxml \n",
    "```\n",
    "\n",
    "*HTML extrated from https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "                      Bank Name             City  ST   CERT  \\\n",
      "0                   Allied Bank         Mulberry  AR     91   \n",
      "1  The Woodbury Banking Company         Woodbury  GA  11297   \n",
      "2        First CornerStone Bank  King of Prussia  PA  35312   \n",
      "3            Trust Company Bank          Memphis  TN   9956   \n",
      "4    North Milwaukee State Bank        Milwaukee  WI  20364   \n",
      "\n",
      "                 Acquiring Institution        Closing Date       Updated Date  \n",
      "0                         Today's Bank  September 23, 2016  November 17, 2016  \n",
      "1                          United Bank     August 19, 2016  November 17, 2016  \n",
      "2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  \n",
      "3           The Bank of Fayette County      April 29, 2016  September 6, 2016  \n",
      "4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Reading HTML\n",
    "tables = pd.read_html(\"../datasets/fdic_failed_bank_list.html\")\n",
    "\n",
    "print(len(tables))\n",
    "\n",
    "fails = tables[0]\n",
    "\n",
    "print(fails.head())\n",
    "# Pandas will insert a line break character '\\' because it has many columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Data Formats\n",
    "\n",
    "Pickle format is only recommended as a short term storage format. pandas object all have a *to_pickle* method that writes the data disk in picle format.\n",
    "```python\n",
    "# DataFrame to pickle\n",
    "frame.to_pickle(\"example\")\n",
    "# Reading\n",
    "pd.read_pickle(\"example\")\n",
    "```\n",
    "\n",
    "### Reading Microsoft Excel Files\n",
    "\n",
    "You can read Excel with pandas.ExcelFile class or pandas.read_excel function. pandas require some packages to do that: `conda install openpyxl xlrd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet names in xlsx file: ['Sheet1']\n",
      "\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "\n",
      "Printing pd.read_excel: \n",
      "   Unnamed: 0  a   b   c   d message\n",
      "0           0  1   2   3   4   hello\n",
      "1           1  5   6   7   8   world\n",
      "2           2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# Create an instance with xlsx file path\n",
    "xlsx = pd.ExcelFile(\"../datasets/ex1.xlsx\")\n",
    "\n",
    "print(f\"Sheet names in xlsx file: {xlsx.sheet_names}\")\n",
    "\n",
    "# Reading data stored in a sheet with 'parse'\n",
    "# selecting index_col\n",
    "sheet = xlsx.parse(sheet_name=\"Sheet1\", index_col=0)\n",
    "print(f\"\\n{sheet}\")\n",
    "\n",
    "# The alternative option is pd.read_excel \n",
    "frame = pd.read_excel(\"../datasets/ex1.xlsx\", sheet_name=\"Sheet1\")\n",
    "print(f\"\\nPrinting pd.read_excel: \\n{frame}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
